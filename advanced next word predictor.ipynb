{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 19, 100)           277900    \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 150)               150600    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2779)              419629    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 848129 (3.24 MB)\n",
      "Trainable params: 848129 (3.24 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/6\n",
      "404/404 [==============================] - 17s 35ms/step - loss: 6.4482 - accuracy: 0.0497\n",
      "Epoch 2/6\n",
      "404/404 [==============================] - 18s 44ms/step - loss: 5.9855 - accuracy: 0.0611\n",
      "Epoch 3/6\n",
      "404/404 [==============================] - 17s 42ms/step - loss: 5.8054 - accuracy: 0.0681\n",
      "Epoch 4/6\n",
      "404/404 [==============================] - 17s 41ms/step - loss: 5.6025 - accuracy: 0.0762\n",
      "Epoch 5/6\n",
      "404/404 [==============================] - 17s 43ms/step - loss: 5.3478 - accuracy: 0.0970\n",
      "Epoch 6/6\n",
      "404/404 [==============================] - 17s 42ms/step - loss: 5.0839 - accuracy: 0.1132\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import regex as re\n",
    "import tkinter as tk\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "\n",
    "# Load the text data from a file\n",
    "with open(\"data2.txt\", \"r\", encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "input_sequences = []\n",
    "for sentence in data.split('\\n'):\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    for i in range(1, len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i + 1])\n",
    "\n",
    "max_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "padded_input_sequence = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "x = padded_input_sequence[:, :-1]\n",
    "y = padded_input_sequence[:, -1]\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_len - 1))\n",
    "model.add(LSTM(150))\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=10)\n",
    "\n",
    "# Function to correct typos\n",
    "def correct_typos(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = [spell.correction(word) for word in text.split()]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# GUI prediction function with typo correction\n",
    "def predict_words():\n",
    "    input_word = input_word_entry.get()\n",
    "    corrected_word = correct_typos(input_word)\n",
    "    input_word_entry.delete(0, tk.END)  # Clear the input field\n",
    "    input_word_entry.insert(0, corrected_word)  # Update input field with corrected word\n",
    "\n",
    "    num_predictions = 3\n",
    "    text = corrected_word\n",
    "\n",
    "    for i in range(num_predictions):\n",
    "        token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "        padded_token_text = pad_sequences([token_text], maxlen=max_len - 1, padding=\"pre\")\n",
    "        predictions_for_input = model.predict(padded_token_text, verbose=0)\n",
    "        predicted_index = np.argmax(predictions_for_input, axis=1)[0]\n",
    "        prediction_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "        if prediction_word:\n",
    "            prediction_labels[i].config(text=prediction_word, fg='black')  # Update prediction labels\n",
    "\n",
    "\n",
    "# Setup tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Word Prediction\")\n",
    "root.configure(bg='lightblue')\n",
    "\n",
    "input_word_label = tk.Label(root, text=\"Input Word:\", font=(\"Palatino\", 24, \"bold\"), fg=\"blue\")\n",
    "input_word_label.pack()\n",
    "input_word_entry = tk.Entry(root, font=(\"Palatino\", 22), bg=\"lightgray\", width=60)\n",
    "input_word_entry.pack()\n",
    "\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_words, font=(\"Palatino\", 18, \"bold\"), bg='black', fg='white')\n",
    "predict_button.pack()\n",
    "\n",
    "prediction_labels = [tk.Label(root, text=\"\", font=(\"Palatino\", 24, \"italic\"), fg='darkblue') for _ in range(3)]\n",
    "for label in prediction_labels:\n",
    "    label.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_4 (Embedding)     (None, 19, 100)           277900    \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, 150)               37650     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 2779)              419629    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 735179 (2.80 MB)\n",
      "Trainable params: 735179 (2.80 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "404/404 [==============================] - 6s 12ms/step - loss: 6.4490 - accuracy: 0.0514\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 5s 13ms/step - loss: 5.8180 - accuracy: 0.0722\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 8s 19ms/step - loss: 5.3848 - accuracy: 0.0999\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 8s 20ms/step - loss: 4.9649 - accuracy: 0.1254\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 7s 17ms/step - loss: 4.5509 - accuracy: 0.1516\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 7s 16ms/step - loss: 4.1311 - accuracy: 0.1922\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 8s 19ms/step - loss: 3.7243 - accuracy: 0.2369\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 7s 17ms/step - loss: 3.3266 - accuracy: 0.3062\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 7s 17ms/step - loss: 3.0052 - accuracy: 0.3677\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 7s 17ms/step - loss: 2.6210 - accuracy: 0.4471\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN\n",
    "\n",
    "# Define the RNN model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_len - 1))\n",
    "model.add(SimpleRNN(150))  # Using SimpleRNN instead of LSTM\n",
    "model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "model.fit(x, y, epochs=10)\n",
    "\n",
    "# Function to correct typos\n",
    "def correct_typos(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = [spell.correction(word) for word in text.split()]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# GUI prediction function with typo correction\n",
    "def predict_words():\n",
    "    input_word = input_word_entry.get()\n",
    "    corrected_word = correct_typos(input_word)\n",
    "    input_word_entry.delete(0, tk.END)  # Clear the input field\n",
    "    input_word_entry.insert(0, corrected_word)  # Update input field with corrected word\n",
    "\n",
    "    num_predictions = 3\n",
    "    text = corrected_word\n",
    "\n",
    "    for i in range(num_predictions):\n",
    "        token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "        padded_token_text = pad_sequences([token_text], maxlen=max_len - 1, padding=\"pre\")\n",
    "        predictions_for_input = model.predict(padded_token_text, verbose=0)\n",
    "        predicted_index = np.argmax(predictions_for_input, axis=1)[0]\n",
    "        prediction_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "        if prediction_word:\n",
    "            prediction_labels[i].config(text=prediction_word, fg='black')  # Update prediction labels\n",
    "\n",
    "\n",
    "# Setup tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Word Prediction\")\n",
    "root.configure(bg='lightblue')\n",
    "\n",
    "input_word_label = tk.Label(root, text=\"Input Word:\", font=(\"Palatino\", 24, \"bold\"), fg=\"blue\")\n",
    "input_word_label.pack()\n",
    "input_word_entry = tk.Entry(root, font=(\"Palatino\", 22), bg=\"lightgray\", width=60)\n",
    "input_word_entry.pack()\n",
    "\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_words, font=(\"Palatino\", 18, \"bold\"), bg='black', fg='white')\n",
    "predict_button.pack()\n",
    "\n",
    "prediction_labels = [tk.Label(root, text=\"\", font=(\"Palatino\", 24, \"italic\"), fg='darkblue') for _ in range(3)]\n",
    "for label in prediction_labels:\n",
    "    label.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
      "     ---------------------------------------- 0.0/131.1 kB ? eta -:--:--\n",
      "     --- ------------------------------------ 10.2/131.1 kB ? eta -:--:--\n",
      "     ----------- ------------------------- 41.0/131.1 kB 487.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 131.1/131.1 kB 1.1 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.20.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB 3.0 MB/s eta 0:00:00\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.2.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suren\\appdata\\roaming\\python\\python311\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.38.1-py3-none-any.whl (8.5 MB)\n",
      "   ---------------------------------------- 0.0/8.5 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 1.0/8.5 MB 30.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.0/8.5 MB 38.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.0/8.5 MB 46.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 7.8/8.5 MB 45.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.5/8.5 MB 42.0 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.20.3-py3-none-any.whl (330 kB)\n",
      "   ---------------------------------------- 0.0/330.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 330.1/330.1 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.6/269.6 kB 17.3 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------------------ --- 2.0/2.2 MB 43.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 35.2 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB 4.5 MB/s eta 0:00:00\n",
      "Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Downloading fsspec-2024.2.0-py3-none-any.whl (170 kB)\n",
      "   ---------------------------------------- 0.0/170.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 170.9/170.9 kB ? eta 0:00:00\n",
      "Installing collected packages: tqdm, safetensors, fsspec, filelock, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed filelock-3.13.1 fsspec-2024.2.0 huggingface-hub-0.20.3 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.38.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c711a25c314f4f9fb1f6129d1b7e3667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\suren\\AppData\\Roaming\\Python\\Python311\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\suren\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3bde64d86c54e96bd5d83f3a6232216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d284215c1f04196a8f986592bdf2372",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c27cad38eab4babb76f3cc8f4e21a6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd1c24c52184891b90099bd53082acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import regex as re\n",
    "import tkinter as tk\n",
    "from spellchecker import SpellChecker\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "\n",
    "# Function to correct typos\n",
    "def correct_typos(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = [spell.correction(word) for word in text.split()]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Load pre-trained model\n",
    "model = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to predict the next word using BERT\n",
    "def predict_next_word(text):\n",
    "    # Tokenize input and convert to ids\n",
    "    input_ids = tokenizer.encode(text, return_tensors='tf')\n",
    "    \n",
    "    # Create masked input ids\n",
    "    masked_input_ids = input_ids.numpy()\n",
    "    masked_input_ids[0, -1] = tokenizer.mask_token_id  # Mask the last token\n",
    "    masked_input_ids = tf.constant(masked_input_ids)\n",
    "\n",
    "    # Predict the masked token with BERT\n",
    "    predictions = model(masked_input_ids)[0]\n",
    "    \n",
    "    # Get the index of the masked token\n",
    "    masked_index = np.where(masked_input_ids == tokenizer.mask_token_id)[1][0]\n",
    "    \n",
    "    # Get the top 3 token predictions of the masked token\n",
    "    predicted_index = np.argsort(predictions[0, masked_index, :])[-3:]\n",
    "    predicted_tokens = tokenizer.convert_ids_to_tokens(predicted_index)\n",
    "    \n",
    "    return predicted_tokens[::-1]  # Return predictions in descending order of probability\n",
    "\n",
    "# GUI prediction function with typo correction\n",
    "def predict_words():\n",
    "    input_word = input_word_entry.get()\n",
    "    corrected_word = correct_typos(input_word)\n",
    "    input_word_entry.delete(0, tk.END)  # Clear the input field\n",
    "    input_word_entry.insert(0, corrected_word)  # Update input field with corrected word\n",
    "\n",
    "    text = corrected_word + ' ' + tokenizer.mask_token  # Add mask token at the end\n",
    "    predicted_words = predict_next_word(text)\n",
    "    \n",
    "    for i, prediction_word in enumerate(predicted_words):\n",
    "        prediction_labels[i].config(text=prediction_word, fg='black')  # Update prediction labels\n",
    "\n",
    "# Setup tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Word Prediction with BERT\")\n",
    "root.configure(bg='lightblue')\n",
    "\n",
    "input_word_label = tk.Label(root, text=\"Input Word:\", font=(\"Palatino\", 24, \"bold\"), fg=\"blue\")\n",
    "input_word_label.pack()\n",
    "input_word_entry = tk.Entry(root, font=(\"Palatino\", 22), bg=\"lightgray\", width=60)\n",
    "input_word_entry.pack()\n",
    "\n",
    "predict_button = tk.Button(root, text=\"Predict\", command=predict_words, font=(\"Palatino\", 18, \"bold\"), bg='black', fg='white')\n",
    "predict_button.pack()\n",
    "\n",
    "prediction_labels = [tk.Label(root, text=\"\", font=(\"Palatino\", 24, \"italic\"), fg='darkblue') for _ in range(3)]\n",
    "for label in prediction_labels:\n",
    "    label.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
