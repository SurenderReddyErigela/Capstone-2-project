{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "404/404 [==============================] - 13s 29ms/step - loss: 6.4443 - accuracy: 0.0507\n",
      "Epoch 2/10\n",
      "404/404 [==============================] - 12s 31ms/step - loss: 5.9864 - accuracy: 0.0627\n",
      "Epoch 3/10\n",
      "404/404 [==============================] - 12s 30ms/step - loss: 5.7810 - accuracy: 0.0711\n",
      "Epoch 4/10\n",
      "404/404 [==============================] - 12s 30ms/step - loss: 5.5352 - accuracy: 0.0875\n",
      "Epoch 5/10\n",
      "404/404 [==============================] - 12s 29ms/step - loss: 5.2757 - accuracy: 0.1077\n",
      "Epoch 6/10\n",
      "404/404 [==============================] - 12s 30ms/step - loss: 5.0290 - accuracy: 0.1239\n",
      "Epoch 7/10\n",
      "404/404 [==============================] - 12s 29ms/step - loss: 4.7939 - accuracy: 0.1436\n",
      "Epoch 8/10\n",
      "404/404 [==============================] - 11s 28ms/step - loss: 4.5585 - accuracy: 0.1605\n",
      "Epoch 9/10\n",
      "404/404 [==============================] - 12s 31ms/step - loss: 4.3293 - accuracy: 0.1777\n",
      "Epoch 10/10\n",
      "404/404 [==============================] - 12s 30ms/step - loss: 4.1065 - accuracy: 0.1978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForMaskedLM.\n",
      "\n",
      "All the weights of TFBertForMaskedLM were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "from spellchecker import SpellChecker\n",
    "from transformers import BertTokenizer, TFBertForMaskedLM\n",
    "\n",
    "# Load the text data from a file\n",
    "with open(\"data2.txt\", \"r\", encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# Word Frequency Analysis\n",
    "word_freq = {}\n",
    "for word, freq in tokenizer.word_counts.items():\n",
    "    word_freq[word] = freq\n",
    "\n",
    "input_sequences = []\n",
    "for sentence in data.split('\\n'):\n",
    "    tokenized_sentence = tokenizer.texts_to_sequences([sentence])[0]\n",
    "\n",
    "    for i in range(1, len(tokenized_sentence)):\n",
    "        input_sequences.append(tokenized_sentence[:i + 1])\n",
    "\n",
    "max_len = max([len(x) for x in input_sequences])\n",
    "\n",
    "padded_input_sequence = pad_sequences(input_sequences, maxlen=max_len, padding='pre')\n",
    "\n",
    "x = padded_input_sequence[:, :-1]\n",
    "y = padded_input_sequence[:, -1]\n",
    "\n",
    "# Define the LSTM model\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(len(tokenizer.word_index) + 1, 100, input_length=max_len - 1))\n",
    "lstm_model.add(LSTM(150))\n",
    "lstm_model.add(Dense(len(tokenizer.word_index) + 1, activation='softmax'))\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "lstm_model.fit(x, y, epochs=10)\n",
    "\n",
    "# BERT Tokenizer\n",
    "tokenizer_bert = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model_bert = TFBertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to correct typos\n",
    "def correct_typos(text):\n",
    "    spell = SpellChecker()\n",
    "    corrected_words = [spell.correction(word) for word in text.split()]\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "# GUI prediction function with LSTM\n",
    "def predict_words_lstm():\n",
    "    try:\n",
    "        input_word = input_word_entry.get()\n",
    "        corrected_word = correct_typos(input_word)\n",
    "        input_word_entry.delete(0, tk.END)  # Clear the input field\n",
    "        input_word_entry.insert(0, corrected_word)  # Update input field with corrected word\n",
    "\n",
    "        num_predictions = 3\n",
    "        text = corrected_word\n",
    "\n",
    "        predictions = []\n",
    "\n",
    "        token_text = tokenizer.texts_to_sequences([text])[0]\n",
    "        padded_token_text = pad_sequences([token_text], maxlen=max_len - 1, padding=\"pre\")\n",
    "        predictions_for_input = lstm_model.predict(padded_token_text, verbose=0)\n",
    "\n",
    "        for i in range(num_predictions):\n",
    "            predicted_index = np.argmax(predictions_for_input, axis=1)[0]\n",
    "            prediction_word = tokenizer.index_word[predicted_index]\n",
    "\n",
    "            if prediction_word:\n",
    "                predictions.append(prediction_word)\n",
    "\n",
    "            # Remove the predicted word from the input sequence\n",
    "            token_text.append(predicted_index)\n",
    "            token_text = token_text[-(max_len - 1):]\n",
    "            padded_token_text = pad_sequences([token_text], maxlen=max_len - 1, padding=\"pre\")\n",
    "            predictions_for_input = lstm_model.predict(padded_token_text, verbose=0)\n",
    "\n",
    "        for i, prediction_word in enumerate(predictions):\n",
    "            prediction_labels[i].config(text=prediction_word, fg='black')  # Update prediction labels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during LSTM prediction:\", e)\n",
    "\n",
    "\n",
    "# GUI prediction function with BERT\n",
    "def predict_words_bert():\n",
    "    try:\n",
    "        input_word = input_word_entry.get()\n",
    "        corrected_word = correct_typos(input_word)\n",
    "        input_word_entry.delete(0, tk.END)  # Clear the input field\n",
    "        input_word_entry.insert(0, corrected_word)  # Update input field with corrected word\n",
    "\n",
    "        text = corrected_word + ' ' + tokenizer_bert.mask_token  # Add mask token at the end\n",
    "        predicted_words = predict_next_word(text)\n",
    "\n",
    "        # Filter out tokens equal to \"\n",
    "        predicted_words = [word for word in predicted_words if word != '\"']\n",
    "\n",
    "        for i, prediction_word in enumerate(predicted_words[:3]):\n",
    "            prediction_labels[i].config(text=prediction_word, fg='black')  # Update prediction labels\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred during BERT prediction:\", e)\n",
    "\n",
    "\n",
    "# Function to predict the next word using BERT\n",
    "def predict_next_word(text):\n",
    "    # Tokenize input and convert to ids\n",
    "    input_ids = tokenizer_bert.encode(text, return_tensors='tf')\n",
    "    \n",
    "    # Create masked input ids\n",
    "    masked_input_ids = input_ids.numpy()\n",
    "    masked_input_ids[0, -1] = tokenizer_bert.mask_token_id  # Mask the last token\n",
    "    masked_input_ids = tf.constant(masked_input_ids)\n",
    "\n",
    "    # Predict the masked token with BERT\n",
    "    predictions = model_bert(masked_input_ids)[0]\n",
    "    \n",
    "    # Get the index of the masked token\n",
    "    masked_index = np.where(masked_input_ids == tokenizer_bert.mask_token_id)[1][0]\n",
    "    \n",
    "    # Get the top 3 token predictions of the masked token\n",
    "    predicted_index = np.argsort(predictions[0, masked_index, :])[-3:]\n",
    "    predicted_tokens = tokenizer_bert.convert_ids_to_tokens(predicted_index)\n",
    "    \n",
    "    return predicted_tokens[::-1]  # Return predictions in descending order of probability\n",
    "\n",
    "# Setup tkinter GUI\n",
    "root = tk.Tk()\n",
    "root.title(\"Word Prediction\")\n",
    "root.configure(bg='lightblue')\n",
    "\n",
    "input_word_label = tk.Label(root, text=\"Input Word:\", font=(\"Palatino\", 24, \"bold\"), fg=\"blue\")\n",
    "input_word_label.pack()\n",
    "input_word_entry = tk.Entry(root, font=(\"Palatino\", 22), bg=\"lightgray\", width=60)\n",
    "input_word_entry.pack()\n",
    "\n",
    "predict_button_lstm = tk.Button(root, text=\"Predict LSTM\", command=predict_words_lstm, font=(\"Palatino\", 18, \"bold\"), bg='black', fg='white')\n",
    "predict_button_lstm.pack()\n",
    "\n",
    "predict_button_bert = tk.Button(root, text=\"Predict BERT\", command=predict_words_bert, font=(\"Palatino\", 18, \"bold\"), bg='black', fg='white')\n",
    "predict_button_bert.pack()\n",
    "\n",
    "prediction_labels = [tk.Label(root, text=\"\", font=(\"Palatino\", 24, \"italic\"), fg='darkblue') for _ in range(3)]\n",
    "for label in prediction_labels:\n",
    "    label.pack()\n",
    "\n",
    "root.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
